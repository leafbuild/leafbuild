use crate::format::format_file;

use super::ws_path;
use log::*;
use std::error::Error;
use syn::Ident;

use serde::{Deserialize, Serialize};

use proc_macro2::{Span, TokenStream};
use quote::{quote, ToTokens, TokenStreamExt};

#[derive(Serialize, Deserialize, Debug)]
pub struct ConstToken {
    name: String,
    kind: String,
    text: String,
}

#[derive(Serialize, Deserialize, Debug)]
pub struct Token {
    name: String,
    kind: String,
    regex: String,
}

#[derive(Serialize, Deserialize, Debug)]
pub enum Child {
    Single(String, String),
    Multiple(String, String),
    First(String, String),
    Last(String, String),
    Optional(String, String),
}

impl Child {
    // fn get_sytax_element_name(&self) -> &String {
    //     match self {
    //         Self::Single(s, ..)
    //         | Self::Multiple(s, ..)
    //         | Self::First(s, ..)
    //         | Self::Last(s, ..)
    //         | Self::Optional(s, ..) => s,
    //     }
    // }

    // fn get_getter_name_fragment(&self) -> &String {
    //     match self {
    //         Self::Single(_, s, ..)
    //         | Self::Multiple(_, s, ..)
    //         | Self::First(_, s, ..)
    //         | Self::Last(_, s, ..)
    //         | Self::Optional(_, s, ..) => s,
    //     }
    // }
}

#[derive(Serialize, Deserialize, Debug)]
pub struct Node {
    name: String,
    kind: String,
    children: Vec<Child>,
}

#[derive(Serialize, Deserialize, Debug)]
pub struct Grammar {
    const_tokens: Vec<ConstToken>,
    tokens: Vec<Token>,
    nodes: Vec<Node>,
}

pub fn generate_grammar() -> Result<(), Box<dyn Error>> {
    let p = ws_path!("crates" / "leafbuild-syntax" / "src" / "grammar.ron");
    info!("Generating grammar from {:?}", p);

    let contents = std::fs::read_to_string(p)?;

    let grammar = ron::from_str(&contents)?;
    let Grammar {
        ref const_tokens,
        ref tokens,
        nodes: _,
    } = grammar;

    let lexer = Lexer {
        tokens,
        const_tokens,
    };
    // let syn_tree = SynTree { nodes };

    {
        let s = quote! {#lexer}.to_string();
        let path = ws_path!("crates" / "leafbuild-syntax" / "src" / "lexer.rs");
        std::fs::write(
            &path,
            format!("{}\n{}", "// @generated by xtask generate-grammar", s),
        )?;
        info!("Wrote {:?}, now formatting", &path);

        format_file(&path)?;

        info!("Formatted {:?}", path);
    }

    {
        let s = grammar.syntax_kind_module().to_string();
        let path = ws_path!("crates" / "leafbuild-syntax" / "src" / "syntax_kind_new.rs");
        std::fs::write(
            &path,
            format!("{}\n{}", "// @generated by xtask generate-grammar", s),
        )?;
        info!("Wrote {:?}, now formatting", &path);

        format_file(&path)?;

        info!("Formatted {:?}", path);
    }

    {
        let s = grammar.syn_tree_implementation().to_string();
        let path =
            ws_path!("crates" / "leafbuild-syntax" / "src" / "syn_tree" / "implementation.rs");
        std::fs::write(
            &path,
            format!("{}\n{}", "// @generated by xtask generate-grammar", s),
        )?;
        info!("Wrote {:?}, now formatting", &path);

        format_file(&path)?;

        info!("Formatted {:?}", path);
    }

    Ok(())
}

#[derive(Debug)]
pub struct Lexer<'a> {
    tokens: &'a [Token],
    const_tokens: &'a [ConstToken],
}

impl ToTokens for Token {
    fn to_tokens(&self, tokens: &mut TokenStream) {
        let Token {
            ref name,
            kind: _,
            ref regex,
        } = self;

        let name_ident = Ident::new(&name, Span::call_site());
        let regex = raw_str_literal(regex);

        tokens.append_all(quote! {
            #[regex(#regex)]
            #name_ident,
        });
    }
}

impl ToTokens for ConstToken {
    fn to_tokens(&self, tokens: &mut TokenStream) {
        let ConstToken {
            ref name, ref text, ..
        } = self;
        let name_ident = Ident::new(name, Span::call_site());

        tokens.append_all(quote! {
            #[token(#text)]
            #name_ident,
        })
    }
}

impl<'a> ToTokens for Lexer<'a> {
    fn to_tokens(&self, tokens: &mut TokenStream) {
        struct MatchTokens<'a>(&'a [Token]);
        struct MatchConstTokens<'a>(&'a [ConstToken]);

        impl<'a> ToTokens for MatchTokens<'a> {
            fn to_tokens(&self, tokens: &mut TokenStream) {
                tokens.append_all(
                    self.0
                        .iter()
                        .map(|it| {
                            let name = Ident::new(&it.name, Span::call_site());
                            quote! {Tk::#name => T![#name],}
                        })
                        .flatten(),
                )
            }
        }

        impl<'a> ToTokens for MatchConstTokens<'a> {
            fn to_tokens(&self, tokens: &mut TokenStream) {
                tokens.append_all(
                    self.0
                        .iter()
                        .map(|it| {
                            let name = Ident::new(&it.name, Span::call_site());
                            let matcher: TokenStream =
                                if it.text.chars().any(|it| "([{)]}".contains(it)) {
                                    format!(r#"'{}'"#, it.text).parse().unwrap()
                                } else if it.text == "\n" {
                                    r#"'\n'"#.parse().unwrap()
                                } else {
                                    it.text.parse().unwrap()
                                };
                            quote! {Tk::#name => T![#matcher],}
                        })
                        .flatten(),
                )
            }
        }

        let Lexer {
            tokens: token_list,
            const_tokens,
        } = self;

        let tokens_match = MatchTokens(token_list);
        let const_tokens_match = MatchConstTokens(const_tokens);

        tokens.append_all(quote! {
            use crate::parser::Span;
            use crate::syntax_kind::SyntaxKind;
            use crate::T;
            use leafbuild_stdx::Let;
            use logos::Logos;
            use std::convert::TryInto;
            use std::ops::Range;

            #[derive(Logos, Copy, Clone, Debug, PartialEq, PartialOrd, Eq)]
            pub enum Tk {
                #(#token_list)*
                #(#const_tokens)*

                #[error]
                // fix for https://github.com/maciejhirsz/logos/issues/180
                #[regex(r"/\*([^*]|\*+[^*/])*\*?")]
                Error,
            }

            impl From<Tk> for SyntaxKind {
                fn from(tk: Tk) -> Self {
                    use SyntaxKind::*;
                    match tk {
                        #tokens_match
                        #const_tokens_match
                        Tk::Error => ERROR,
                    }
                }
            }

            #[allow(missing_debug_implementations)]
            pub struct Lexer<'a> {
                lexer: logos::SpannedIter<'a, Tk>,
            }
            impl<'a> Lexer<'a> {
                pub(crate) fn new(s: &'a str) -> Self {
                    let lexer = Tk::lexer(s).spanned();
                    Self { lexer }
                }
            }
            impl<'a> Iterator for Lexer<'a> {
                type Item = (SyntaxKind, Span);
                fn next(&mut self) -> Option<Self::Item> {
                    self.lexer.next().map(|(token, span)| {
                        (
                            token.into(),
                            span.let_(|it| -> Range<u32> {
                                it.start.try_into().unwrap()..it.end.try_into().unwrap()
                            })
                            .into(),
                        )
                    })
                }
            }

        });
    }
}

#[derive(Debug)]
pub struct SynTree {
    nodes: Vec<Node>,
}

fn raw_str_literal(s: &'_ str) -> ::proc_macro2::TokenStream {
    let mut depth = 0;
    let mut cur_depth = None;
    let mut chars = s.chars().peekable();
    loop {
        match (cur_depth, chars.peek()) {
            (_, Some('#')) => {}
            (Some(d), _) => depth = depth.max(d),
            _ => {}
        }
        match (chars.next(), cur_depth) {
            (None, _) => break,
            (Some('"'), _) => cur_depth = Some(0),
            (Some('#'), Some(ref mut d)) => *d += 1,
            (Some(_), _) => cur_depth = None,
        }
    }
    format!(
        "r{0:#^raw_depth$}\"{wrapped}\"{0:#^raw_depth$}",
        "",
        wrapped = s,
        raw_depth = depth + 1,
    )
    .parse()
    .unwrap()
}

enum ElementType {
    Node,
    Token,
}

impl Grammar {
    fn syntax_kind_module(&self) -> TokenStream {
        struct SKind<'a>(&'a str);
        struct SKindTokenNameMatch<'a>(Vec<(SKind<'a>, &'a str)>);

        impl<'a> ToTokens for SKind<'a> {
            fn to_tokens(&self, tokens: &mut TokenStream) {
                let name = Ident::new(self.0, Span::call_site());
                tokens.append_all(quote! {#name, })
            }
        }

        impl<'a> ToTokens for SKindTokenNameMatch<'a> {
            fn to_tokens(&self, tokens: &mut TokenStream) {
                tokens.append_all(
                    self.0
                        .iter()
                        .map(|it| {
                            let name = Ident::new(it.0 .0, Span::call_site());
                            let token_name = it.1;
                            quote! {#name => #token_name, }
                        })
                        .flatten(),
                )
            }
        }

        let kinds = self
            .const_tokens
            .iter()
            .map(|it| it.kind.as_str())
            .chain(self.tokens.iter().map(|it| it.kind.as_str()))
            .chain(self.nodes.iter().map(|it| it.kind.as_str()))
            .map(SKind);

        let kinds_token_name_match = SKindTokenNameMatch(
            self.const_tokens
                .iter()
                .map(|it| (SKind(it.kind.as_str()), it.text.as_str()))
                .chain(
                    self.tokens
                        .iter()
                        .map(|it| (SKind(it.kind.as_str()), it.name.as_str())),
                )
                .collect(),
        );

        struct TokenMacroMatches<'a>(Vec<(SKind<'a>, &'a str)>);

        impl<'a> ToTokens for TokenMacroMatches<'a> {
            fn to_tokens(&self, tokens: &mut TokenStream) {
                tokens.append_all(
                    self.0
                        .iter()
                        .map(|it| {
                            let matcher: TokenStream =
                                if it.1.chars().any(|it| "([{)]}".contains(it)) {
                                    format!(r#"'{}'"#, it.1).parse().unwrap()
                                } else if it.1 == "\n" {
                                    r#"'\n'"#.parse().unwrap()
                                } else {
                                    it.1.parse().unwrap()
                                };
                            let kind = Ident::new(&it.0 .0, Span::call_site());
                            quote! {[#matcher] => { $crate::SyntaxKind:: #kind };}
                        })
                        .flatten(),
                )
            }
        }

        let token_macro_matches = TokenMacroMatches(
            self.const_tokens
                .iter()
                .map(|it| (SKind(it.kind.as_str()), it.text.as_str()))
                .chain(
                    self.tokens
                        .iter()
                        .map(|it| (SKind(it.kind.as_str()), it.name.as_str())),
                )
                .collect(),
        );

        quote! {
            #![allow(missing_docs)]
            #[derive(Debug, Clone, Copy, PartialEq, Eq, PartialOrd, Ord, Hash)]
            #[allow(non_camel_case_types, clippy::upper_case_acronyms)]
            #[repr(u16)]
            pub enum SyntaxKind {
                ERROR,
                EOF,
                TOMBSTONE,
                #(#kinds)*
            }

            impl From<SyntaxKind> for rowan::SyntaxKind {
                fn from(kind: SyntaxKind) -> Self {
                    Self(kind.into())
                }
            }

            impl From<&SyntaxKind> for rowan::SyntaxKind {
                fn from(kind: &SyntaxKind) -> Self {
                    Self(kind.into())
                }
            }

            #[allow(clippy::fallible_impl_from)]
            impl From<u16> for SyntaxKind {
                fn from(i: u16) -> Self {
                    assert!(i <= Self::ROOT as u16);
                    // only usage of unsafe code
                    #[allow(unsafe_code)]
                    unsafe {
                        std::mem::transmute::<u16, Self>(i)
                    }
                }
            }

            impl From<SyntaxKind> for u16 {
                fn from(kind: SyntaxKind) -> Self {
                    kind as Self
                }
            }

            impl From<&SyntaxKind> for u16 {
                fn from(kind: &SyntaxKind) -> Self {
                    (*kind).into()
                }
            }

            impl SyntaxKind {
                pub(crate) fn token_name(&self) -> &'static str {
                    use SyntaxKind::*;
                    match self {
                        #kinds_token_name_match
                        _ => "",
                    }
                }
            }

            #[macro_export]
            #[doc = "A nice way of getting [`SyntaxKind`]s from tokens in rust itself."]
            macro_rules! T {
                #token_macro_matches
            }
        }
    }

    fn element_type(&self, name: &str) -> Option<ElementType> {
        if self
            .tokens
            .iter()
            .map(|it| &it.name)
            .chain(self.const_tokens.iter().map(|it| &it.name))
            .any(|it| it == name)
        {
            Some(ElementType::Token)
        } else if self.nodes.iter().map(|it| &it.name).any(|it| it == name) {
            Some(ElementType::Node)
        } else {
            None
        }
    }

    fn syn_tree_implementation(&self) -> TokenStream {
        let mut stream = quote! {};

        fn append_for_token(stream: &mut TokenStream, name: &str, kind: &str) {
            let name = Ident::new(name, Span::call_site());
            let kind = Ident::new(kind, Span::call_site());

            stream.append_all(quote! {
                #[derive(Clone, Debug)]
                #[repr(transparent)]
                pub struct #name {
                    syntax: SyntaxToken,
                }

                impl AstToken for #name {
                    const KIND: SyntaxKind = SyntaxKind::#kind;

                    #[allow(unsafe_code)]
                    unsafe fn new(syntax: SyntaxToken) -> Self {
                        Self { syntax }
                    }

                    fn get_text(&self) -> &str {
                        self.syntax.text()
                    }
                }
            });
        }

        fn append_for_node(stream: &mut TokenStream, grammar: &Grammar, node: &Node) {
            struct Children<'a>(&'a Grammar, &'a [Child], &'a str);

            impl<'a> ToTokens for Children<'a> {
                fn to_tokens(&self, tokens: &mut TokenStream) {
                    self.1.iter().for_each(|child| {
                        match child {
                            Child::Single(name, fn_name_fragment) => {
                                let name_ident = Ident::new(name, Span::call_site());
                                let fn_name = Ident::new(
                                    &format!("get_{}", fn_name_fragment),
                                    Span::call_site(),
                                );

                                let element_type = self.0.element_type(name);

                                let helper_fn_name = match element_type {
                                    Some(ElementType::Token) => "get_single_tok",
                                    Some(ElementType::Node) => "get_single",
                                    None => {
                                        error!(
                                            "Cannot find child element of name '{}', in '{}'",
                                            name, self.2
                                        );
                                        panic!("Unknown reference")
                                    }
                                };

                                let helper_fn_name = Ident::new(helper_fn_name, Span::call_site());

                                tokens.append_all(quote! {
                                    pub fn #fn_name(&self) -> #name_ident {
                                        super::#helper_fn_name(&self.syntax)
                                    }
                                });
                            }
                            Child::Multiple(name, fn_name_fragment) => {
                                let name_ident = Ident::new(name, Span::call_site());
                                let fn_name = Ident::new(
                                    &format!("get_{}_iter", fn_name_fragment),
                                    Span::call_site(),
                                );

                                let element_type = self.0.element_type(name);

                                let helper_fn_name = match element_type {
                                    Some(ElementType::Token) => "get_multiple_tok",
                                    Some(ElementType::Node) => "get_multiple",
                                    None => {
                                        error!(
                                            "Cannot find child element of name '{}', in '{}'",
                                            name, self.2
                                        );
                                        panic!("Unknown reference")
                                    }
                                };

                                let helper_fn_name = Ident::new(helper_fn_name, Span::call_site());

                                tokens.append_all(quote! {
                                    pub fn #fn_name(&self) -> impl Iterator<Item=#name_ident> {
                                        super::#helper_fn_name(&self.syntax)
                                    }
                                });
                            }
                            Child::First(name, fn_name_fragment) => {
                                let name_ident = Ident::new(name, Span::call_site());
                                let fn_name = Ident::new(
                                    &format!("get_{}", fn_name_fragment),
                                    Span::call_site(),
                                );

                                let element_type = self.0.element_type(name);

                                let helper_fn_name = match element_type {
                                    Some(ElementType::Token) => "get_first_tok",
                                    Some(ElementType::Node) => "get_first",
                                    None => {
                                        error!(
                                            "Cannot find child element of name '{}', in '{}'",
                                            name, self.2
                                        );
                                        panic!("Unknown reference")
                                    }
                                };

                                let helper_fn_name = Ident::new(helper_fn_name, Span::call_site());

                                tokens.append_all(quote! {
                                    pub fn #fn_name(&self) -> #name_ident {
                                        super::#helper_fn_name(&self.syntax)
                                    }
                                });
                            }
                            Child::Last(name, fn_name_fragment) => {
                                let name_ident = Ident::new(name, Span::call_site());
                                let fn_name = Ident::new(
                                    &format!("get_{}", fn_name_fragment),
                                    Span::call_site(),
                                );

                                let element_type = self.0.element_type(name);

                                let helper_fn_name = match element_type {
                                    Some(ElementType::Token) => "get_last_tok",
                                    Some(ElementType::Node) => "get_last",
                                    None => {
                                        error!(
                                            "Cannot find child element of name '{}', in '{}'",
                                            name, self.2
                                        );
                                        panic!("Unknown reference")
                                    }
                                };

                                let helper_fn_name = Ident::new(helper_fn_name, Span::call_site());

                                tokens.append_all(quote! {
                                    pub fn #fn_name(&self) -> #name_ident {
                                        super::#helper_fn_name(&self.syntax)
                                    }
                                });
                            }
                            Child::Optional(name, fn_name_fragment) => {
                                let name_ident = Ident::new(name, Span::call_site());
                                let fn_name = Ident::new(
                                    &format!("get_{}_opt", fn_name_fragment),
                                    Span::call_site(),
                                );
                                let element_type = self.0.element_type(name);

                                let helper_fn_name = match element_type {
                                    Some(ElementType::Token) => "get_opt_tok",
                                    Some(ElementType::Node) => "get_opt",
                                    None => {
                                        error!(
                                            "Cannot find child element of name '{}', in '{}'",
                                            name, self.2
                                        );
                                        panic!("Unknown reference")
                                    }
                                };

                                let helper_fn_name = Ident::new(helper_fn_name, Span::call_site());

                                tokens.append_all(quote! {
                                    pub fn #fn_name(&self) -> Option<#name_ident> {
                                        super::#helper_fn_name(&self.syntax)
                                    }
                                });
                            }
                        };
                    });
                }
            }

            let name = Ident::new(&node.name, Span::call_site());
            let kind = Ident::new(&node.kind, Span::call_site());
            let children = Children(grammar, &node.children, &node.name);

            stream.append_all(quote! {
                #[derive(Debug, Clone)]
                pub struct #name {
                    syntax: SyntaxNode,
                }

                impl AstNode for #name {
                    const KIND: SyntaxKind = SyntaxKind::#kind;

                    #[allow(unsafe_code)]
                    unsafe fn new(syntax: SyntaxNode) -> Self {
                        Self { syntax }
                    }

                    fn syntax(&self) -> &SyntaxNode {
                        &self.syntax
                    }
                }

                impl #name {
                    #children
                }
            })
        }

        fn mark_section(stream: &mut TokenStream, section: &str) {
            let ts = format!("//\n// {}\n//", section)
                .parse::<TokenStream>()
                .unwrap();
            stream.append_all(ts);
        }

        stream.append_all(quote! {
            #![allow(missing_docs)]
            use super::{
                AstNode, AstToken, SyntaxKind, SyntaxNode, SyntaxToken,
            };
        });

        mark_section(&mut stream, "Const tokens");
        self.const_tokens
            .iter()
            .for_each(|it| append_for_token(&mut stream, &it.name, &it.kind));

        mark_section(&mut stream, "Tokens");
        self.tokens
            .iter()
            .for_each(|it| append_for_token(&mut stream, &it.name, &it.kind));

        mark_section(&mut stream, "Nodes");
        self.nodes
            .iter()
            .for_each(|node| append_for_node(&mut stream, self, node));
        stream
    }
}
